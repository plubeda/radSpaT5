{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d91b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "print(msize(model.shared) / msize(model))\n",
    "print(msize(model.lm_head) / msize(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69828bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "print(msize(model.shared) / msize(model))\n",
    "print(msize(model.lm_head) / msize(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c8f5a",
   "metadata": {},
   "source": [
    "## Selecting the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_CORPUS = 'Spanish_T5'\n",
    "path_spanish_medical_texts  = 'my_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "cnt_spa = Counter()\n",
    "\n",
    "path_selected = path_spanish_medical_texts\n",
    "\n",
    "for file in os.listdir(path_selected):\n",
    "    with open(os.path.join(path_selected, file), 'r') as ftext:\n",
    "        text = ftext.read()\n",
    "        \n",
    "    cnt_spa.update(tokenizer.encode(text))\n",
    "\n",
    "print(len(cnt_spa), len(cnt_spa)/tokenizer.vocab_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total vocab\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "for top in 10_000, 20_000, 30_000:\n",
    "    print(top, sum(v for k, v in cnt_spa.most_common(top)) / sum(cnt_spa.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_id, freq in cnt_spa.most_common(30):\n",
    "    print(tokenizer.decode(word_id), freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbdc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"\"\n",
    "\n",
    "# using the top 30k tokens\n",
    "for t in cnt_spa.most_common(30_000):\n",
    "    s += \"{}\\t{}\\t{}\\n\".format(t[0], tokenizer.decode(t[0]), t[1])\n",
    "\n",
    "\n",
    "with open(f'30_000_ESP_'+NAME_CORPUS+'_T5small.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4904a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = set(range(1000)) # 1K of top tokens of the original tokenizer (just in case)\n",
    "\n",
    "# Top 30K of the Spanish vocabulary\n",
    "for i, (k, v) in enumerate(cnt_spa.most_common(30_000)):\n",
    "    \n",
    "    if len(new_tokens) == 29_900:\n",
    "        print(i, 'Spanish tokens are included')\n",
    "        break\n",
    "    \n",
    "    if k not in new_tokens:\n",
    "        new_tokens.add(k)\n",
    "        \n",
    "# The 100 special tokens that T5 uses\n",
    "for t in range(tokenizer.vocab_size - 100, tokenizer.vocab_size):\n",
    "    new_tokens.add(t)\n",
    "\n",
    "print(len(new_tokens))\n",
    "kept_ids = sorted(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62b2c9",
   "metadata": {},
   "source": [
    "## Updating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size = len(kept_ids)\n",
    "new_emb = torch.nn.Embedding(new_size, model.shared.embedding_dim)\n",
    "new_head = torch.nn.Linear(in_features=model.lm_head.in_features, out_features=new_size, bias=False)\n",
    "for new_id, old_id in enumerate(kept_ids):\n",
    "    new_emb.weight.data[new_id] = model.shared.weight.data[old_id]\n",
    "    new_head.weight.data[new_id] = model.lm_head.weight.data[old_id]\n",
    "\n",
    "model.shared.weight = new_emb.weight\n",
    "model.lm_head.weight = new_head.weight\n",
    "model.config.__dict__['vocab_size'] = new_size\n",
    "model.config.__dict__['_name_or_path'] = NAME_CORPUS + '/es5-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a442ec",
   "metadata": {},
   "source": [
    "## Updating the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5fbc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece_model_pb2 as spmp\n",
    "\n",
    "smp = tokenizer.sp_model.serialized_model_proto()\n",
    "m = spmp.ModelProto()\n",
    "m.ParseFromString(smp)\n",
    "\n",
    "print('the loaded model has pieces:', len(m.pieces))\n",
    "new_pieces = [m.pieces[idx] for idx in kept_ids]\n",
    "\n",
    "print('the new pieces:', len(new_pieces))\n",
    "\n",
    "# replace the content of the first 30K pieces\n",
    "for i, p in enumerate(new_pieces):\n",
    "    m.pieces[i].piece = p.piece\n",
    "    m.pieces[i].score = p.score\n",
    "    m.pieces[i].type = p.type\n",
    "\n",
    "# drop the remaining pieces\n",
    "n = len(new_pieces)\n",
    "\n",
    "for i in trange(len(m.pieces) - n):\n",
    "    m.pieces.pop(len(m.pieces) - 1)\n",
    "\n",
    "print(len(m.pieces))\n",
    "\n",
    "with open(NAME_CORPUS + '_es5-small.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())\n",
    "\n",
    "new_tokenizer = T5Tokenizer(NAME_CORPUS + '_es5-small.model', extra_ids=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = T5Tokenizer(NAME_CORPUS+'_es5-small.model', extra_ids=100)\n",
    "new_tokenizer.save_pretrained(NAME_CORPUS + '_small/espt5-' +NAME_CORPUS+ 'small')\n",
    "model.save_pretrained('espt5-' +NAME_CORPUS+ 'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb55b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
