{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbae3a1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6474c7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808af8a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "print(msize(model.shared) / msize(model))\n",
    "print(msize(model.lm_head) / msize(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc33f3d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "print(msize(model.shared) / msize(model))\n",
    "print(msize(model.lm_head) / msize(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd7701",
   "metadata": {},
   "source": [
    "## Selecting the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea01bf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NAME_CORPUS = 'Spanish_T5'\n",
    "path_spanish_medical_texts  = 'my_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0ad2d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "cnt_spa = Counter()\n",
    "\n",
    "path_selected = path_spanish_medical_texts\n",
    "\n",
    "for file in os.listdir(path_selected):\n",
    "    with open(os.path.join(path_selected, file), 'r') as ftext:\n",
    "        text = ftext.read()\n",
    "        \n",
    "    cnt_spa.update(tokenizer.encode(text))\n",
    "\n",
    "print(len(cnt_spa), len(cnt_spa)/tokenizer.vocab_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31888d0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Total vocab\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07468e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for top in 10_000, 20_000, 30_000:\n",
    "    print(top, sum(v for k, v in cnt_spa.most_common(top)) / sum(cnt_spa.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0e6d4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word_id, freq in cnt_spa.most_common(30):\n",
    "    print(tokenizer.decode(word_id), freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e31c0f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=\"\"\n",
    "\n",
    "# using the top 30k tokens\n",
    "for t in cnt_spa.most_common(30_000):\n",
    "    s += \"{}\\t{}\\t{}\\n\".format(t[0], tokenizer.decode(t[0]), t[1])\n",
    "\n",
    "\n",
    "with open(f'30_000_ESP_'+NAME_CORPUS+'_T5small.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a158e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tokens = set(range(1000)) # 1K of top tokens of the original tokenizer (just in case)\n",
    "\n",
    "# Top 30K of the Spanish vocabulary\n",
    "for i, (k, v) in enumerate(cnt_spa.most_common(30_000)):\n",
    "    \n",
    "    if len(new_tokens) == 29_900:\n",
    "        print(i, 'Spanish tokens are included')\n",
    "        break\n",
    "    \n",
    "    if k not in new_tokens:\n",
    "        new_tokens.add(k)\n",
    "        \n",
    "# The 100 special tokens that T5 uses\n",
    "for t in range(tokenizer.vocab_size - 100, tokenizer.vocab_size):\n",
    "    new_tokens.add(t)\n",
    "\n",
    "print(len(new_tokens))\n",
    "kept_ids = sorted(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1ad53",
   "metadata": {},
   "source": [
    "## Updating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add46f1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_size = len(kept_ids)\n",
    "new_emb = torch.nn.Embedding(new_size, model.shared.embedding_dim)\n",
    "new_head = torch.nn.Linear(in_features=model.lm_head.in_features, out_features=new_size, bias=False)\n",
    "for new_id, old_id in enumerate(kept_ids):\n",
    "    new_emb.weight.data[new_id] = model.shared.weight.data[old_id]\n",
    "    new_head.weight.data[new_id] = model.lm_head.weight.data[old_id]\n",
    "\n",
    "model.shared.weight = new_emb.weight\n",
    "model.lm_head.weight = new_head.weight\n",
    "model.config.__dict__['vocab_size'] = new_size\n",
    "model.config.__dict__['_name_or_path'] = NAME_CORPUS + '/es5-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e4e79",
   "metadata": {},
   "source": [
    "## Updating the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9791ad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22251/1095395163.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msentencepiece_model_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialized_model_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/22_Generate_conclusion/T5/01_Prepare_models_T5_SPA/sentencepiece_model_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDESCRIPTOR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   values=[\n\u001b[0;32m---> 33\u001b[0;31m     _descriptor.EnumValueDescriptor(\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UNIGRAM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mserialized_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[0m\n\u001b[1;32m    778\u001b[0m                 \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                 options=None, serialized_options=None, create_key=None):\n\u001b[0;32m--> 780\u001b[0;31m       \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m       \u001b[0;31m# There is no way we can build a complete EnumValueDescriptor with the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0;31m# given parameters (the name of the Enum is not known, for example).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import sentencepiece_model_pb2 as spmp\n",
    "\n",
    "smp = tokenizer.sp_model.serialized_model_proto()\n",
    "m = spmp.ModelProto()\n",
    "m.ParseFromString(smp)\n",
    "\n",
    "print('the loaded model has pieces:', len(m.pieces))\n",
    "new_pieces = [m.pieces[idx] for idx in kept_ids]\n",
    "\n",
    "print('the new pieces:', len(new_pieces))\n",
    "\n",
    "# replace the content of the first 30K pieces\n",
    "for i, p in enumerate(new_pieces):\n",
    "    m.pieces[i].piece = p.piece\n",
    "    m.pieces[i].score = p.score\n",
    "    m.pieces[i].type = p.type\n",
    "\n",
    "# drop the remaining pieces\n",
    "n = len(new_pieces)\n",
    "\n",
    "for i in trange(len(m.pieces) - n):\n",
    "    m.pieces.pop(len(m.pieces) - 1)\n",
    "\n",
    "print(len(m.pieces))\n",
    "\n",
    "with open(NAME_CORPUS + '_es5-small.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())\n",
    "\n",
    "new_tokenizer = T5Tokenizer(NAME_CORPUS + '_es5-small.model', extra_ids=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2117ebb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tokenizer = T5Tokenizer(NAME_CORPUS+'_es5-small.model', extra_ids=100)\n",
    "new_tokenizer.save_pretrained(NAME_CORPUS + '_small/espt5-' +NAME_CORPUS+ 'small')\n",
    "model.save_pretrained('espt5-' +NAME_CORPUS+ 'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f356d93",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
